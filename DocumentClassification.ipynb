{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Text pre-processing and exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from os import listdir\n",
    "from os.path import isfile, join    \n",
    "import sklearn.datasets as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here you should put the directory path\n",
    "path='D:\\\\documents\\\\users\\\\nogahm\\\\Downloads\\\\ohsumed-first-20000-docs\\\\testCorpus'\n",
    "\n",
    "dirpath=path\n",
    "trainDirs=[]\n",
    "testDirs=[]\n",
    "trainFiles=[]\n",
    "testFiles=[]\n",
    "# for table\n",
    "Category=[]\n",
    "FileName=[]\n",
    "Path=[]\n",
    "Text=[]\n",
    "classNumOfDocs=[]\n",
    "docsInfo=[] #[fileName,cleanText,class\n",
    "\n",
    "stopWords=(stopwords.words('english'))\n",
    "stopWords += ['.', ',', '(', ')','%',';',':']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test dirs\n",
    "trainDirs=os.listdir(dirpath+'\\\\training')\n",
    "testDirs=os.listdir(dirpath+'\\\\test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process\n",
    "def cleanText(text):\n",
    "    # tokenize\n",
    "    tokenized = word_tokenize(text)\n",
    "    # remove stop words\n",
    "    filtered_sentence = [w for w in tokenized if not w in stopWords]\n",
    "    # stemming\n",
    "    ps = PorterStemmer()\n",
    "    for i in range(len(filtered_sentence)):\n",
    "        filtered_sentence[i] = ps.stem(filtered_sentence[i])\n",
    "    # print(\"aaa\")\n",
    "    return \" \".join(filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test dirs\n",
    "for dir in trainDirs:\n",
    "    for file in os.listdir(dirpath+'\\\\training\\\\'+dir):\n",
    "        trainFiles.append(file)\n",
    "        Category.append(dir)\n",
    "        FileName.append(file)\n",
    "        Path.append(dirpath+'\\\\training\\\\'+dir+'\\\\'+file)\n",
    "        # Open a file: fileReader\n",
    "        fileReader = open(dirpath+'\\\\training\\\\'+dir+'\\\\'+file,mode='r')\n",
    "        # read all lines at once\n",
    "        text = fileReader.read().lower()\n",
    "        # close the file\n",
    "        fileReader.close()\n",
    "\n",
    "        cleanedText=cleanText(text)\n",
    "        docsInfo.append([file,cleanedText,dir,'training'])\n",
    "\n",
    "for dir in testDirs:\n",
    "    for file in os.listdir(dirpath+'\\\\test\\\\'+dir):\n",
    "        testFiles.append(file)\n",
    "        Category.append(dir)\n",
    "        FileName.append(file)\n",
    "        Path.append(dirpath+'\\\\test\\\\'+dir+'\\\\'+file)\n",
    "        # Open a file: fileReader\n",
    "        fileReader = open(dirpath+'\\\\test\\\\'+dir+'\\\\'+file,mode='r')\n",
    "        # read all lines at once\n",
    "        text = fileReader.read().lower()\n",
    "        # close the file\n",
    "        fileReader.close()\n",
    "\n",
    "        cleanedText = cleanText(text)\n",
    "        docsInfo.append([file, cleanedText, dir, 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of categories:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class  #OfFiles\n0   C01       929\n1   C02       391\n2   C03       135\n3   C04      2630\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>term 1</th>\n",
       "      <th>term 2</th>\n",
       "      <th>term 3</th>\n",
       "      <th>term 4</th>\n",
       "      <th>term 5</th>\n",
       "      <th>term 6</th>\n",
       "      <th>term 7</th>\n",
       "      <th>term 8</th>\n",
       "      <th>term 9</th>\n",
       "      <th>term 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C01</td>\n",
       "      <td>(patient, 2145)</td>\n",
       "      <td>(infect, 1457)</td>\n",
       "      <td>(case, 551)</td>\n",
       "      <td>(group, 524)</td>\n",
       "      <td>(diseas, 516)</td>\n",
       "      <td>(studi, 513)</td>\n",
       "      <td>(treatment, 494)</td>\n",
       "      <td>(use, 464)</td>\n",
       "      <td>(less, 407)</td>\n",
       "      <td>(clinic, 407)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02</td>\n",
       "      <td>(patient, 794)</td>\n",
       "      <td>(infect, 765)</td>\n",
       "      <td>(viru, 457)</td>\n",
       "      <td>(human, 348)</td>\n",
       "      <td>(hiv, 285)</td>\n",
       "      <td>(cell, 278)</td>\n",
       "      <td>(studi, 273)</td>\n",
       "      <td>(immunodefici, 264)</td>\n",
       "      <td>(diseas, 243)</td>\n",
       "      <td>(case, 240)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C03</td>\n",
       "      <td>(infect, 193)</td>\n",
       "      <td>(patient, 188)</td>\n",
       "      <td>(malaria, 118)</td>\n",
       "      <td>(antibodi, 84)</td>\n",
       "      <td>(case, 81)</td>\n",
       "      <td>(parasit, 80)</td>\n",
       "      <td>(respons, 73)</td>\n",
       "      <td>(studi, 73)</td>\n",
       "      <td>(day, 72)</td>\n",
       "      <td>(treatment, 72)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C04</td>\n",
       "      <td>(patient, 6719)</td>\n",
       "      <td>(cell, 3681)</td>\n",
       "      <td>(tumor, 3340)</td>\n",
       "      <td>(cancer, 2452)</td>\n",
       "      <td>(carcinoma, 1957)</td>\n",
       "      <td>(case, 1658)</td>\n",
       "      <td>(studi, 1655)</td>\n",
       "      <td>(use, 1493)</td>\n",
       "      <td>(treatment, 1308)</td>\n",
       "      <td>(diseas, 1270)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>term 1</th>\n",
       "      <th>term 2</th>\n",
       "      <th>term 3</th>\n",
       "      <th>term 4</th>\n",
       "      <th>term 5</th>\n",
       "      <th>term 6</th>\n",
       "      <th>term 7</th>\n",
       "      <th>term 8</th>\n",
       "      <th>term 9</th>\n",
       "      <th>term 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C01</td>\n",
       "      <td>(patient, 2145)</td>\n",
       "      <td>(infect, 1457)</td>\n",
       "      <td>(case, 551)</td>\n",
       "      <td>(group, 524)</td>\n",
       "      <td>(diseas, 516)</td>\n",
       "      <td>(studi, 513)</td>\n",
       "      <td>(treatment, 494)</td>\n",
       "      <td>(use, 464)</td>\n",
       "      <td>(less, 407)</td>\n",
       "      <td>(clinic, 407)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C02</td>\n",
       "      <td>(patient, 794)</td>\n",
       "      <td>(infect, 765)</td>\n",
       "      <td>(viru, 457)</td>\n",
       "      <td>(human, 348)</td>\n",
       "      <td>(hiv, 285)</td>\n",
       "      <td>(cell, 278)</td>\n",
       "      <td>(studi, 273)</td>\n",
       "      <td>(immunodefici, 264)</td>\n",
       "      <td>(diseas, 243)</td>\n",
       "      <td>(case, 240)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C03</td>\n",
       "      <td>(infect, 193)</td>\n",
       "      <td>(patient, 188)</td>\n",
       "      <td>(malaria, 118)</td>\n",
       "      <td>(antibodi, 84)</td>\n",
       "      <td>(case, 81)</td>\n",
       "      <td>(parasit, 80)</td>\n",
       "      <td>(respons, 73)</td>\n",
       "      <td>(studi, 73)</td>\n",
       "      <td>(day, 72)</td>\n",
       "      <td>(treatment, 72)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C04</td>\n",
       "      <td>(patient, 6719)</td>\n",
       "      <td>(cell, 3681)</td>\n",
       "      <td>(tumor, 3340)</td>\n",
       "      <td>(cancer, 2452)</td>\n",
       "      <td>(carcinoma, 1957)</td>\n",
       "      <td>(case, 1658)</td>\n",
       "      <td>(studi, 1655)</td>\n",
       "      <td>(use, 1493)</td>\n",
       "      <td>(treatment, 1308)</td>\n",
       "      <td>(diseas, 1270)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get number of classes and number of docs for each class\n",
    "docInfoDF=pd.DataFrame(docsInfo, columns=['fileName','cleanText','class','group'])\n",
    "allClasses=np.unique(docInfoDF['class'])\n",
    "print ('# of categories: ',allClasses.size)\n",
    "numOfFiles=[]\n",
    "words_per_class = []\n",
    "termsFreqPerClass=[]\n",
    "for currClass in allClasses:\n",
    "    temp=(docInfoDF['class']).value_counts()[currClass]\n",
    "    numOfFiles.append([currClass, temp])\n",
    "    #get terms distibution\n",
    "    currFiles=docInfoDF.loc[docInfoDF['class']==currClass]\n",
    "    termsFreq=pd.Series(\" \".join(currFiles['cleanText']).split()).value_counts()[:10]\n",
    "    # termsFreqPerClass.append([currClass,zip(termsFreq.index,termsFreq)])\n",
    "    zipedFreq=zip(termsFreq.index, termsFreq)\n",
    "    zippedList=list(zipedFreq)\n",
    "    termsFreqPerClass.append([currClass]+zippedList)\n",
    "\n",
    "# print number of files prt category\n",
    "numOfFilesDF=pd.DataFrame(numOfFiles,columns=['Class','#OfFiles'])\n",
    "print(numOfFilesDF)\n",
    "# print terms frequency\n",
    "pd.DataFrame(termsFreqPerClass, columns=['Class','term 1','term 2','term 3','term 4','term 5','term 6','term 7','term 8','term 9','term 10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2. Document classification: <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature extraction methods: TF-IDF, Bag Of Words\n",
    "machine learning models: SVM, Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best\n",
    "best_accuracy = 0.0\n",
    "best_fe = ''\n",
    "best_classifier = ''\n",
    "best_corpus_type = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=docInfoDF.loc[docInfoDF['group']=='training']\n",
    "trainSetText=train['cleanText'].tolist()\n",
    "trainSetClass=train['class'].tolist()\n",
    "test=docInfoDF.loc[docInfoDF['group']=='test']\n",
    "testSetText=train['cleanText'].tolist()\n",
    "testSetClass=train['class'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(extractM,machineLearningM ): \n",
    "            pipeline = Pipeline([('vect', extractM), ('clf', machineLearningM)])\n",
    "            parameters = {'vect__max_df': np.arange(0.05, 0.55, 0.05),'clf__alpha': np.arange(0.01, 0.11, 0.01)}\n",
    "            gs_clf = GridSearchCV(pipeline, parameters, n_jobs=1)\n",
    "            gs_clf = gs_clf.fit(trainSetText, trainSetClass)\n",
    "            prediction = gs_clf.predict(testSetText)\n",
    "            accuracy = metrics.accuracy_score(testSetClass, prediction)\n",
    "            print(accuracy)\n",
    "            return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF AND SVM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.642896627971\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "#TF-IDF & SVM\n",
    "print('TF-IDF AND SVM')\n",
    "accuracy = classify(TfidfVectorizer(),SGDClassifier() )\n",
    "results.append(['TF-IDF' , 'SVM', accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF & Naïve Bayes\n",
    "print('TF-IDF AND Naïve Bayes')\n",
    "accuracy = classify(TfidfVectorizer(),MultinomialNB() )\n",
    "results.append(['TF-IDF' , 'Naïve Bayes', accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words & SVM\n",
    "print('Bag of Words AND SVM')\n",
    "accuracy = classify(CountVectorizer(),SGDClassifier() )\n",
    "results.append(['Bag of Words' , 'SVM', accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words & Naïve Bayes\n",
    "print('Bag of Words AND Naïve Bayes')\n",
    "accuracy = classify(CountVectorizer(),MultinomialNB() )\n",
    "results.append(['Bag of Words' , 'Naïve Bayes', accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.DataFrame(results,columns=['extraction method','machine learning method', 'accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
